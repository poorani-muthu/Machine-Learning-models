clear all

% Multivariable_Linear_Regression 

% Loading Data
train=readtable("train.csv");

% X: independent variables matrix
X=[train.distance_traveled(1:1500),train.total_fare(1:1500)]%,train.tip(1:100),train.miscellaneous_fees(1:100)];
%Normalizing data
X=normalize(X);

% Y: dependent variable
Y=train.total_fare(1:1500);

%defining theta and b (parameters)
theta=ones(2,1);
b=1;

%cost
iterations=200;
cost=zeros(iterations,1);
cost_lst = zeros(1,iterations);

%learning rate
a=0.1;  

m = size(X,1);

for i =1:iterations
    %Prediction of data
    H = X*theta+b;

    % calculating gradient and parameter degradation

    gradients = (2/m)*(X'*(H-Y));
    theta = theta - a *gradients;
    b=b-a*sum(H-Y)/m;

    % calculaitng cost
    cost_value = 1/(2*m)*((H - Y).^2); 

    %Calculate the loss for each training instance
    total = 0;
    for j =1:m
        total = total+ cost_value(j,1);
    end

    % total cost of iteration
    cost_lst(i)=total;
end

% cost reduction with iteration

subplot(2,1,1)
plot(1:200,cost_lst(1:200))
disp(theta')
disp(b')


%test data set prediction and accuracy


Ytest=train.total_fare(101:151)
Xtest=[train.distance_traveled(101:151),train.total_fare(101:151)];
Xtest=normalize(Xtest);
H= Xtest*theta+b
accuracy= 1-mean((H-Ytest)./Ytest);
accuracy=accuracy*100


# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a52U8hT2H1bAxpBuD2qCe3k9T6BG0TOD
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import OneHotEncoder

from scipy.special import softmax
onehot_encoder = OneHotEncoder(sparse_output=False)
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import balanced_accuracy_score

iris = load_iris()
data, labels = iris.data, iris.target

res = train_test_split(data, labels,
                       train_size=0.7,
                       test_size=0.3
                      )
train_data, test_data, train_labels, test_labels = res

def loss(X, Y, W):

    Z = - X @ W
    N = X.shape[0]
    loss = 1/N * (np.trace(X @ W @ Y.T) + np.sum(np.log(np.sum(np.exp(Z), axis=1))))
    return loss

def gradient(X, Y, W, mu):

    Z = - X @ W
    P = softmax(Z, axis=1)
    N = X.shape[0]
    gd = 1/N * (X.T @ (Y - P)) + 2 * mu * W
    return gd

def gradient_descent(X, Y, max_iter=1000, eta=0.1, mu=0.01):

    Y_onehot = onehot_encoder.fit_transform(Y.reshape(-1,1))
    global W
    W = np.zeros((X.shape[1], Y_onehot.shape[1]))
    step = 0
    step_lst = []
    loss_lst = []
    W_lst = []

    while step < max_iter:
        step += 1
        W -= eta * gradient(X, Y_onehot, W, mu)
        step_lst.append(step)
        W_lst.append(W)
        loss_lst.append(loss(X, Y_onehot, W))

    df = pd.DataFrame({
        'step': step_lst,
        'loss': loss_lst
    })
    return df, W

class Multiclass:
    def fit(self, X, Y):
        self.loss_steps, self.W = gradient_descent(X, Y)

    def loss_plot(self):
        return self.loss_steps.plot(
            x='step',
            y='loss',
            xlabel='step',
            ylabel='loss'
        )

    def predict(self, H):
        Z = - H @ self.W
        P = softmax(Z, axis=1)
        return (np.argmax(P, axis=1))

X = train_data
Y = train_labels

# fit model
model = Multiclass()
model.fit(X, Y)

# plot loss, prediction and comparison with actual value
model.loss_plot()
print(model.predict(X))
print(model.predict(X) == Y)

#-----------------------------------------------------------------testing the model------------------------------------------------------------------------
X_test= test_data
Y_test= test_labels

# fit model

model = Multiclass()
model.fit(X_test, Y_test)

Z_p = - X_test @ W
P= softmax(Z_p, axis=1)
Y_pred=np.argmax(P, axis=1)
print(Y_pred)

print(Y_pred == Y_test)

print('Model accuracy: {:.3f}'.format(balanced_accuracy_score(Y_test,Y_pred)))

